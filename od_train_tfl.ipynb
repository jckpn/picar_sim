{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/cuda-11.2/bin/\"\n",
    "if \"LD_LIBRARY_PATH\" in os.environ:\n",
    "    os.environ[\"LD_LIBRARY_PATH\"] += os.pathsep + \"/usr/local/cuda-11.2/lib64\"\n",
    "else:\n",
    "    os.environ[\"LD_LIBRARY_PATH\"] = (\n",
    "        \"/usr/local/cuda-11.2/lib64/\" + os.pathsep + \"/usr/local/cuda/lib64\"\n",
    "    )\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-11.2\"\n",
    "os.environ[\"CUDADIR\"] = \"/usr/local/cuda-11.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dino/miniconda3/envs/tfl/bin:/home/dino/.vscode-server/bin/b58957e67ee1e712cebf466b995adf4c5307b2bd/bin/remote-cli:/home/dino/.local/bin:/home/dino/miniconda3/condabin:/home/dino/.nvm/versions/node/v16.15.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3/bin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.3/libnvvp:/mnt/c/Program Files/Zulu/zulu-8/bin:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/NVIDIA/CUDNN/v8.8/bin:/mnt/c/Program Files/ImageMagick-7.1.1-Q16-HDRI:/mnt/c/Program Files/Oculus/Support/oculus-runtime:/mnt/c/Python38/Scripts:/mnt/c/Python38:/mnt/c/Program Files (x86)/Python38-32/Scripts:/mnt/c/Program Files (x86)/Python38-32:/mnt/c/Program Files (x86)/Common Files/Intel/Shared Libraries/redist/intel64/compiler:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/dotnet:/mnt/c/Program Files (x86)/dotnet:/mnt/c/ffmpeg:/mnt/c/Program Files/Topaz Labs/Gigapixel AI for Video:/mnt/c/Program Files/Microsoft SQL Server/130/Tools/Binn:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files (x86)/WinMerge:/mnt/c/luarocks:/mnt/c/luarocks/systree/bin:/mnt/c/Program Files (x86)/Natural Docs:/mnt/c/Program Files (x86)/Graphviz/bin:/mnt/c/Program Files (x86)/Graphviz:/mnt/c/Program Files (x86)/PlantUML:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Calibre2:/mnt/c/platform-tools:/mnt/c/Program Files/MATLAB/R2023a/runtime/win64:/mnt/c/Program Files/MATLAB/R2023a/bin:/mnt/c/Program Files/apache-maven-3.9.2/bin:/mnt/c/Strawberry/c/bin:/mnt/c/Strawberry/perl/site/bin:/mnt/c/Strawberry/perl/bin:/mnt/c/Program Files (x86)/Yarn/bin:/mnt/c/Program Files/get_iplayer:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/PuTTY:/mnt/c/Program Files/nodejs:/mnt/c/Program Files (x86)/sox-14-4-2:/mnt/c/Program Files/NVIDIA Corporation/Nsight Compute 2023.3.1:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Program Files/Tailscale:/mnt/c/Program Files/mpv.net:/mnt/c/texlive/2023/bin/windows:/mnt/c/Python310/Scripts:/mnt/c/Python310:/mnt/c/Users/Dino/anaconda3:/mnt/c/Users/Dino/anaconda3/Library/mingw-w64/bin:/mnt/c/Users/Dino/anaconda3/Library/usr/bin:/mnt/c/Users/Dino/anaconda3/Library/bin:/mnt/c/Users/Dino/anaconda3/Scripts:/mnt/c/Users/Dino/AppData/Local/Programs/Python/Python38-32/Scripts:/mnt/c/Users/Dino/AppData/Local/Programs/Python/Python38-32:/mnt/c/Users/Dino/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Dino/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Dino/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/Dino/AppData/Local/Microsoft/WindowsApps:/mnt/c/Program Files/JetBrains/PyCharm Community Edition 2020.2.3/bin:/mnt/c/Users/Dino/.dotnet/tools:/mnt/c/Program Files (x86)/FreeArc/bin:/mnt/c/Users/Dino/spicetify-cli:/mnt/c/Users/Dino/AppData/Local/JetBrains/Toolbox/scripts:/mnt/c/Users/Dino/AppData/Local/Programs/mpv:/mnt/c/Program Files/Multipass/bin:/mnt/c/Users/Dino/AppData/Local/Yarn/bin:/mnt/c/Users/Dino/AppData/Roaming/npm:/mnt/c/Users/Dino/AppData/Local/Pandoc:/mnt/c/Users/Dino/AppData/Roaming/Python/Scripts:/snap/bin:/usr/local/cuda-11.2/bin/\n",
      "/usr/local/cuda-11.2/lib64/:/usr/local/cuda/lib64\n",
      "/usr/local/cuda-11.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dino/miniconda3/envs/tfl/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/dino/miniconda3/envs/tfl/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/dino/miniconda3/envs/tfl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(os.environ[\"PATH\"])\n",
    "print(os.environ[\"LD_LIBRARY_PATH\"])\n",
    "print(os.environ[\"CUDADIR\"])\n",
    "\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "from tensorflow_examples.lite.model_maker.third_party.efficientdet.keras import train\n",
    "from tensorflow_examples.lite.model_maker.third_party.efficientdet.keras import (\n",
    "    train_lib,\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__.startswith(\"2\")\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__.startswith(\"2\")\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "from absl import logging\n",
    "\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 12:58:58.955837: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-04 12:58:59.270445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:58:59.289461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:58:59.289728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:58:59.830818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:58:59.831172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:58:59.831188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-04 12:58:59.831467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:58:59.831518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 6571 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:26:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    1: \"box\",\n",
    "    2: \"green_light\",\n",
    "    3: \"left_arrow\",\n",
    "    4: \"no_light\",\n",
    "    5: \"person\",\n",
    "    6: \"red_light\",\n",
    "    7: \"right_arrow\",\n",
    "    8: \"tree\",\n",
    "    9: \"unknown_arrow\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count: 4875\n",
      "validation count: 667\n"
     ]
    }
   ],
   "source": [
    "train_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    images_dir=\"label_data/pascal/training/images\",\n",
    "    annotations_dir=\"label_data/pascal/training/Annotations\",\n",
    "    label_map=class_map,\n",
    ")\n",
    "\n",
    "validation_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    images_dir=\"label_data/pascal/validation/images\",\n",
    "    annotations_dir=\"label_data/pascal/validation/Annotations\",\n",
    "    label_map=class_map,\n",
    ")\n",
    "print(f\"train count: {len(train_data)}\")\n",
    "print(f\"validation count: {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 12:59:02.797110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:59:02.797429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:59:02.797710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:59:02.798371: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:59:02.798652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:59:02.799070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:59:02.799535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:59:02.799551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-04 12:59:02.800007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-04 12:59:02.800044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6571 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:26:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "hparams = {\n",
    "    \"input_rand_hflip\": False,\n",
    "    \"jitter_min\": 0.8,\n",
    "    \"jitter_max\": 1.2,\n",
    "    \"autoaugment_policy\": \"v0\",\n",
    "}\n",
    "\n",
    "spec = object_detector.EfficientDetLite1Spec(\n",
    "    model_dir=\"tfl/efficientdet_lite1\",\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    hparams=hparams,\n",
    "    tflite_max_detections=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = object_detector.create(\n",
    "    train_data,\n",
    "    model_spec=spec,\n",
    "    batch_size=batch_size,\n",
    "    train_whole_model=True,\n",
    "    validation_data=validation_data,\n",
    "    epochs=epochs,\n",
    "    do_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, steps_per_epoch, _ = detector._get_dataset_and_steps(\n",
    "    train_data, batch_size, is_training=True\n",
    ")\n",
    "validation_ds, validation_steps, val_json_file = detector._get_dataset_and_steps(\n",
    "    validation_data, batch_size, is_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = detector.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = spec.config\n",
    "config.update(\n",
    "    dict(\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        eval_samples=batch_size * validation_steps,\n",
    "        val_json_file=val_json_file,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    ")\n",
    "\n",
    "train.setup_model(model, config)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"tfl/efficientdet_lite1\"\n",
    "try:\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "    # latest = checkpoint_dir\n",
    "\n",
    "    completed_epochs = int(latest.split(\"/\")[-1].split(\"-\")[1])\n",
    "    model.load_weights(latest)\n",
    "\n",
    "    print(f\"Loaded model from {latest}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    completed_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_callbacks = train_lib.get_callbacks(config.as_dict(), validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_callbacks.append(\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"tfl/efficientdet_lite1/logs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_ds,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=all_callbacks,\n",
    "    initial_epoch=completed_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = \"tfl/efficientdet_lite1/export\"\n",
    "quant_config = QuantizationConfig.for_int8(representative_data=train_data)\n",
    "detector.model = model\n",
    "detector.export(\n",
    "    export_dir,\n",
    "    quantization_config=quant_config,\n",
    "    tflite_filename=\"model.tflite\",\n",
    "    label_filename=\"labelmap.txt\",\n",
    "    export_format=[ExportFormat.TFLITE, ExportFormat.LABEL],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.evaluate_tflite(\"tfl/efficientdet_lite1/export/model.tflite\", validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = \"label_data/test/New folder2/capture/1714486754083_80_35.png\"\n",
    "\n",
    "# input_image = \"label_data/test/New folder2/capture/1714486714290_110_35.png\"\n",
    "\n",
    "# input_image = \"label_data/test/downloads_images/1709570998863_90_35.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "\n",
    "model_path = \"tfl/efficientdet_lite1/export/model.tflite\"\n",
    "\n",
    "# Load the labels into a list\n",
    "classes = [\"???\"] * detector.model_spec.config.num_classes\n",
    "label_map = detector.model_spec.config.label_map\n",
    "for label_id, label_name in label_map.as_dict().items():\n",
    "    classes[label_id - 1] = label_name\n",
    "\n",
    "print(classes)\n",
    "\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "    \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "    original_image = img\n",
    "    resized_img = tf.image.resize(img, input_size)\n",
    "    resized_img = resized_img[tf.newaxis, :]\n",
    "    resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "    return resized_img, original_image\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold):\n",
    "    \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "\n",
    "    signature_fn = interpreter.get_signature_runner()\n",
    "\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # pprint(output_details)\n",
    "\n",
    "    # pprint(interpreter.get_signature_list())\n",
    "\n",
    "    # Feed the input image to the model\n",
    "    output = signature_fn(images=image)\n",
    "    # pprint(output)\n",
    "\n",
    "    output_map = {\"output_0\": 2, \"output_1\": 0, \"output_2\": 3, \"output_3\": 1}\n",
    "\n",
    "    if output_details[0][\"dtype\"] == np.uint8:\n",
    "        for key, i in output_map.items():\n",
    "            output_scale, output_zero_point = output_details[i][\"quantization\"]\n",
    "            print(\n",
    "                f\"output_scale: {output_scale}, output_zero_point: {output_zero_point}\"\n",
    "            )\n",
    "            output[key] = output[key].astype(np.float32)\n",
    "            output[key] = (output[key] - output_zero_point) * output_scale\n",
    "\n",
    "    output[\"output_0\"] = np.round(output[\"output_0\"]).astype(np.int32)\n",
    "    output[\"output_2\"] = np.round(output[\"output_2\"]).astype(np.int32)\n",
    "\n",
    "    # pprint(output)\n",
    "\n",
    "    # Get all outputs from the model\n",
    "    count = int(np.squeeze(output[\"output_0\"]))\n",
    "    scores = np.squeeze(output[\"output_1\"])\n",
    "    classes = np.squeeze(output[\"output_2\"])\n",
    "    boxes = np.squeeze(output[\"output_3\"])\n",
    "\n",
    "    results = []\n",
    "    for i in range(count):\n",
    "        if scores[i] >= threshold:\n",
    "            result = {\n",
    "                \"bounding_box\": boxes[i],\n",
    "                \"class_id\": classes[i],\n",
    "                \"score\": scores[i],\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5):\n",
    "    \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "    # Load the input shape required by the model\n",
    "    _, input_height, input_width, _ = interpreter.get_input_details()[0][\"shape\"]\n",
    "\n",
    "    # Load the input image and preprocess it\n",
    "    preprocessed_image, original_image = preprocess_image(\n",
    "        image_path, (input_height, input_width)\n",
    "    )\n",
    "\n",
    "    # Run object detection on the input image\n",
    "    results = detect_objects(interpreter, preprocessed_image, threshold=threshold)\n",
    "\n",
    "    # Plot the detection results on the input image\n",
    "    original_image_np = original_image.numpy().astype(np.uint8)\n",
    "    for obj in results:\n",
    "        # Convert the object bounding box from relative coordinates to absolute\n",
    "        # coordinates based on the original image resolution\n",
    "        ymin, xmin, ymax, xmax = obj[\"bounding_box\"]\n",
    "        xmin = int(xmin * original_image_np.shape[1])\n",
    "        xmax = int(xmax * original_image_np.shape[1])\n",
    "        ymin = int(ymin * original_image_np.shape[0])\n",
    "        ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "        # Find the class index of the current object\n",
    "        class_id = int(obj[\"class_id\"])\n",
    "\n",
    "        # Draw the bounding box and label on the image\n",
    "        color = [int(c) for c in COLORS[class_id]]\n",
    "        cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "        # Make adjustments to make the label visible for all objects\n",
    "        y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "        label = \"{}: {:.0f}%\".format(classes[class_id], obj[\"score\"] * 100)\n",
    "        cv2.putText(\n",
    "            original_image_np, label, (xmin, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2\n",
    "        )\n",
    "\n",
    "    # Return the final image\n",
    "    original_uint8 = original_image_np.astype(np.uint8)\n",
    "    return original_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(input_image)\n",
    "im.thumbnail((512, 512), Image.LANCZOS)\n",
    "im.save(\"temp.png\", \"PNG\")\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "threshold = 0.3\n",
    "\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    \"temp.png\", interpreter, threshold=threshold\n",
    ")\n",
    "\n",
    "Image.fromarray(detection_result_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
